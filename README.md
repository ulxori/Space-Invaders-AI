# Space Invaders AI

![Project Image](https://user-images.githubusercontent.com/62477191/153773366-a738f16a-1785-4b27-8736-a29fb4d4530c.png)

---

### Table of Contents

- [Description](#description)
- [Technologies](#how-to-use)
- [Results](#license)
- [References](#author-info)

---

## Description
A project designed in order to create AI playing Atari Space Invaders.


#### DDQN

Double Q-Learning is a version of Q-Learning that uses two neural networks. The difference to regular q-Learning is that in DDQN instead of q array we use the model for the prediction, instead of inserting new values in a q array, we perform the fit operation on the network model.  The output of the model will be q-values ​​for a given action. The value of q is calculated using the following formula:

![CodeCogsEqn](https://user-images.githubusercontent.com/62477191/153775640-a39d88e9-8091-4f40-b72f-a5b34b14a310.png)

Double Q-Learning uses two models to ensure better effectiveness. If we only used one model, the same model would be updated every one step and would be used for prediction. Two are used to increase "stability" models. The first (main_model) is updated with each loop of the training loop. The second model (target_model) is updated with a certain frequency, and this model is used to predict future q values. In the project, the target model was updated every 10 episodes.

---
#### Training Data

The game frames are used as training data in project. Based on the images, we are able to determine the location of the bullets, aliens and controlled ship. Images generated by the gym environment simulating the games have a resolution of 210x160 pixels and are in RGB format, which gives an array with the size of 210x160x3. For optimization purposes, images are converted from RGB to grayscale. This allows you to resize the array from 210x160x3 to resize 210x160x1. Then we can cut unnecessary fragments from the images and converted the image size. These operations allowed to reduce the amount of memory used by the program and speed up the process of learning. In the figures below, we can see what frames looked like before and after operations.

###### Before
---

![image](https://user-images.githubusercontent.com/62477191/153773467-4bb6499c-80d3-4344-8718-c3769c8005d3.png)

###### After
---
![image](https://user-images.githubusercontent.com/62477191/153773484-e0a77d5b-3adc-413a-94d9-1b8d51ad8469.png)



[Back To The Top](#space-invaders-ai)

---

## Technologies

 - Python
 - Tensorflow
 - Gym
 - Numpy
 - Matplotlib
 
 
[Back To The Top](#space-invaders-ai)

---



## Results


In the chart below, we can see the effectiveness an agent taking random actions. 

###### Random actions
---
![image](https://user-images.githubusercontent.com/62477191/153773670-3b8b1a7f-3014-4788-b26d-2df3acf7afdc.png)

###### Trained agent.
---
![image](https://user-images.githubusercontent.com/62477191/153773729-c043722f-e62a-400f-a5d3-29b2d3e5168f.png)


As we can see, a trained agent performs much better than an agent taking random actions.


[Back To The Top](#space-invaders-ai)

---
